{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j4Qb7nU6RuC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd\n",
        "import copy as cp\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need 12 min states. So let's define a 4x4 grid having 16 states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM3-Q6zv68Ib"
      },
      "outputs": [],
      "source": [
        "class GridEnvironment(gym.Env):\n",
        "  metadata = { 'render.modes': []}\n",
        "\n",
        "  def __init__(self, observation_space, action_space, max_timesteps, stochasticity=1):\n",
        "    self.observation_space = spaces.Discrete(observation_space)\n",
        "    self.action_space = spaces.Discrete(action_space)\n",
        "    self.max_timesteps = max_timesteps\n",
        "    # self.rewards = np.array([[0, 1, 1, 1],\n",
        "    #         [5, -5, -5, 1],\n",
        "    #         [5, -5, -1, 1],\n",
        "    #         [5, 5, 5, 10]])\n",
        "    # Implemented a simpler reward calculation system later\n",
        "    self.stochasticity = stochasticity\n",
        "\n",
        "  def set_epsilon(self, epsilon):\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def reset(self):\n",
        "    self.timestep = 0\n",
        "    self.agent_pos = [0, 0]\n",
        "    self.goal_pos = [3, 3]\n",
        "    self.state = np.zeros((4, 4))\n",
        "    self.state[tuple(self.agent_pos)] = 1\n",
        "    self.state[tuple(self.goal_pos)] = 0.5\n",
        "    observation = self.state.flatten()\n",
        "    return observation\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    if self.stochasticity != 1:\n",
        "      action = self.get_action_random()\n",
        "\n",
        "    current_pos = cp.deepcopy(self.agent_pos)\n",
        "\n",
        "    if action == 0: # Go left\n",
        "      self.agent_pos[0] -= 1\n",
        "    if action == 1: # Go up\n",
        "      self.agent_pos[1] -= 1\n",
        "    if action == 2: # Go right\n",
        "      self.agent_pos[0] += 1\n",
        "    if action == 3: # Go down\n",
        "      self.agent_pos[1] += 1\n",
        "    \n",
        "    self.agent_pos = list(np.clip(self.agent_pos, 0, 3))\n",
        "    future_pos = self.agent_pos\n",
        "    self.state = np.zeros((4,4))\n",
        "    self.state[tuple(self.agent_pos)] = 1\n",
        "    self.state[tuple(self.goal_pos)] = 0.5\n",
        "    observation = self.state.flatten()\n",
        "\n",
        "    reward = self.get_reward(current_pos, future_pos)\n",
        "\n",
        "    self.timestep += 1\n",
        "    done = True if self.timestep >= self.max_timesteps or self.agent_pos == self.goal_pos else False\n",
        "    info = {}\n",
        "\n",
        "    return observation, reward, done, info, action\n",
        "\n",
        "  # the reward is calculated on the basis of the distance of the\n",
        "  # current_pos and future_pos of the agent from the goal_pos\n",
        "  def get_reward(self, current_pos, future_pos):\n",
        "\n",
        "    current_dist = self.distance_from_goal(current_pos)\n",
        "    final_dist = self.distance_from_goal(future_pos)\n",
        "\n",
        "    if final_dist < current_dist:\n",
        "      return 5\n",
        "    else:\n",
        "      return -5\n",
        "  \n",
        "  def distance_from_goal(self, pos):\n",
        "    x_final, y_final = self.goal_pos\n",
        "    x, y = pos\n",
        "    return math.sqrt( (y_final - y)**2 + (x_final - x)**2 )\n",
        "\n",
        "  def get_action_random(self):\n",
        "    return np.random.choice(self.action_space.n)\n",
        "      \n",
        "\n",
        "  def render(self):\n",
        "    plt.figure()\n",
        "    plt.imshow(self.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function returns the action the agent ðŸ¤– will take"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_action():\n",
        "    return np.random.choice(env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to run the agent and generate results â¬‡ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "iYkVISkn9dJp",
        "outputId": "b380a733-9fc4-45d5-dd7f-b7047b5159ec"
      },
      "outputs": [],
      "source": [
        "def run_agent_and_generate_results():\n",
        "    rewards_sum = 0\n",
        "    data_cell = []\n",
        "    col_headers = ['TimeStep', 'Action', 'Reward', 'Total Rewards', 'Done']\n",
        "    for timestep in range(0,env.max_timesteps):\n",
        "        action = get_action()\n",
        "        observation, reward, done, info, action = env.step(action)\n",
        "        rewards_sum += reward\n",
        "        data_cell.append([timestep, action, reward, rewards_sum, done])\n",
        "        env.render()\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.axis('off')\n",
        "    plt.table(cellText=data_cell, colLabels=col_headers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets initialize our environment with the following parameters â¬‡ï¸ :\n",
        "- observation_space = 16 Discrete\n",
        "- action_space = 4 (Left = 0, Up = 1, Right = 2, Down = 3)\n",
        "- max_timesteps = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "8Rk7dsV99XZ2",
        "outputId": "5f18f174-6fbc-44ec-bb7f-1750925669ff"
      },
      "outputs": [],
      "source": [
        "observation_space = 16\n",
        "action_space = 4\n",
        "max_timesteps = 10\n",
        "\n",
        "env = GridEnvironment(observation_space, action_space, max_timesteps, stochasticity=1)\n",
        "obs = env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the agent in a deterministic environment â¬‡ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.reset()\n",
        "run_agent_and_generate_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the agent in a stochastic environment â¬‡ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridEnvironment(observation_space, action_space, max_timesteps, stochasticity=0.9)\n",
        "obs = env.reset()\n",
        "run_agent_and_generate_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper Functions to help update q_table â¬‡ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get the max q value of the future state of the agent\n",
        "def get_max_q_for_state(pos):\n",
        "    state = get_state_from_position(pos)\n",
        "    if state == env.observation_space.n - 1:\n",
        "        return 0\n",
        "    q_list = q_table[state]\n",
        "    return max(q_list)\n",
        "\n",
        "def get_state_from_position(pos):\n",
        "    X, Y = env.state.shape\n",
        "    pos_x, pos_y = pos\n",
        "    return Y * pos_x + pos_y\n",
        "\n",
        "# Function to calculate q_value\n",
        "def update_q_value(current_pos, future_pos, immediate_reward, action):\n",
        "\n",
        "    state = get_state_from_position(current_pos)\n",
        "\n",
        "    if get_state_from_position(current_pos) == env.observation_space.n - 1:\n",
        "        q_table[state][action] = 0\n",
        "        return\n",
        "\n",
        "    q_value = q_table[state][action] + learning_rate * ( immediate_reward + discount_factor * get_max_q_for_state(future_pos) - q_table[state][action])\n",
        "    q_table[state][action] = q_value\n",
        "\n",
        "\n",
        "def get_action_greedy(table):\n",
        "    state = get_state_from_position(env.agent_pos)\n",
        "    val_list = list(table[state])\n",
        "    action = val_list.index(max(val_list))\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function helps us run an episode and return the results â¬‡ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following function runs an episode\n",
        "def run_episode_q_learn(episode_num, epsilon, env):\n",
        "\n",
        "    rewards_sum = 0\n",
        "    timestep_results = []\n",
        "    \n",
        "    is_episode_successful = False\n",
        "    steps = 0\n",
        "    penalty = 0\n",
        "    for timestep in range(0,env.max_timesteps):\n",
        "\n",
        "        chosen_action = None\n",
        "        if np.random.random() < epsilon:\n",
        "            chosen_action = get_action()\n",
        "        else:\n",
        "            chosen_action = get_action_greedy(q_table)\n",
        "\n",
        "        current_pos = cp.deepcopy(env.agent_pos)\n",
        "        observation, reward, done, info, action = env.step(chosen_action)\n",
        "        future_pos = cp.deepcopy(env.agent_pos)\n",
        "\n",
        "        if reward < 0:\n",
        "            penalty += 1\n",
        "        \n",
        "        # Update q table and current episode outcome\n",
        "        update_q_value(current_pos, future_pos, reward, action)\n",
        "        rewards_sum += reward\n",
        "        steps += 1\n",
        "\n",
        "        # env.render()\n",
        "        if done:\n",
        "            if steps < max_timesteps:\n",
        "                is_episode_successful = True\n",
        "            break\n",
        "            \n",
        "    return timestep_results, rewards_sum, is_episode_successful, steps, penalty, env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementation of Q Learning Algorithm.\n",
        "We need to initialize the following:\n",
        " - Q table. Q(terminal_state, any_action) will always be 0\n",
        " - Step Size / Learning Rate Alpha\n",
        " - Epsilon (start with small value close to 0)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_table = np.zeros((observation_space, action_space))\n",
        "learning_rate = 0.15\n",
        "epsilon = 1.0\n",
        "discount_factor = 0.9\n",
        "n_episodes = 1000\n",
        "decay_rate = 0.995\n",
        "\n",
        "# Initialising pandas dataframes to store results\n",
        "# Only write data directly to the dataframe from the code below\n",
        "time_step_results = pd.DataFrame()\n",
        "\n",
        "# episode_result = pd.DataFrame(columns=ep_res_headers)\n",
        "episode_result = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's begin the training loop âž°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Begin the Training Loop\n",
        "stochasticity = 1.0\n",
        "max_timesteps = 15\n",
        "envir = GridEnvironment(observation_space, action_space, max_timesteps, stochasticity=stochasticity)\n",
        "obs = env.reset()\n",
        "\n",
        "for i in range(0,n_episodes):\n",
        "\n",
        "    envir.reset()\n",
        "\n",
        "    timestep_results, rewards_sum, is_episode_successful, steps, penalty, env = run_episode_q_learn(i, epsilon, envir)\n",
        "    time_step_results = time_step_results.append(timestep_results)\n",
        "\n",
        "    episode_result = episode_result.append({\n",
        "        'Episode #': i,\n",
        "        'Total Rewards': rewards_sum,\n",
        "        'Successful': is_episode_successful,\n",
        "        'Steps': steps,\n",
        "        'Penalty': penalty,\n",
        "        'Epsilon': epsilon\n",
        "    }, ignore_index=True)\n",
        "    epsilon *= decay_rate\n",
        "    envir = env\n",
        "\n",
        "    if i == n_episodes - 1:\n",
        "        print(\"Final q_table is: \\n\", q_table)\n",
        "\n",
        "timestep_file = './timestep_result_train_q_learn_deterministic.xlsx'\n",
        "episode_file = './episode_result_train_q_learn_deterministic.xlsx'\n",
        "\n",
        "if stochasticity < 1:\n",
        "    timestep_file = timestep_file.replace('deterministic', 'stochastic')\n",
        "    episode_file = episode_file.replace('deterministic', 'stochastic')\n",
        "\n",
        "time_step_results.to_excel(timestep_file)\n",
        "episode_result.to_excel(episode_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to evaluate agent\n",
        "def evaluate_agent_qlearn(num, env, print_episode=False):\n",
        "\n",
        "    penalty = 0\n",
        "    rewards_sum = 0\n",
        "    data_cell = []\n",
        "    steps = 0\n",
        "\n",
        "    for i in range(max_timesteps):\n",
        "\n",
        "        if print_episode:\n",
        "            env.render()\n",
        "        action = get_action_greedy(q_table)\n",
        "        observation, reward, done, info, action = env.step(action)\n",
        "        if reward < 0:\n",
        "            penalty += 1\n",
        "\n",
        "        rewards_sum += reward\n",
        "        steps += 1\n",
        "\n",
        "        data_cell.append([num, i, action, reward, rewards_sum, done])\n",
        "        if done:\n",
        "            if print_episode:\n",
        "                env.render()\n",
        "            break\n",
        "        \n",
        "    return data_cell, rewards_sum, penalty, steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run a single episode and visualise if the agent ðŸ¤– reaches the goal ðŸ¥… position. The ðŸ¤– choses greedy action each time in this episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "envir.reset()\n",
        "evaluate_agent_qlearn(1, envir, print_episode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's evaluate few more episodes by letting the ðŸ¤– chose greedy policy each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_q_learn(env, DETERMINISTIC = True):\n",
        "    envir.reset()\n",
        "    n_episodes = 10\n",
        "    timestep_data = []\n",
        "    episode_data = pd.DataFrame()\n",
        "\n",
        "    for i in range(1, n_episodes+1):\n",
        "        env.reset()\n",
        "        data_cell, rewards_sum, penalty, steps = evaluate_agent_qlearn(i, env)\n",
        "        timestep_data.append(data_cell)\n",
        "        episode_data = episode_data.append({\n",
        "            'Episode #': i, \n",
        "            'Total rewards': rewards_sum, \n",
        "            'Penalty': penalty,\n",
        "            'Steps': steps\n",
        "            }, ignore_index=True)\n",
        "\n",
        "        if i == n_episodes:\n",
        "            print(episode_data.head)\n",
        "\n",
        "\n",
        "    episode_file = './eval_ep_data_q_learn_deterministic.xlsx'\n",
        "    if not DETERMINISTIC:\n",
        "        episode_file = episode_file.replace('deterministic', 'stochastic')\n",
        "    episode_data.to_excel(episode_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_q_learn(envir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's train the agent ðŸ¤– in a stochastic environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_table = np.zeros((observation_space, action_space))\n",
        "learning_rate = 0.15\n",
        "epsilon = 1.0\n",
        "discount_factor = 0.9\n",
        "n_episodes = 2000\n",
        "decay_rate = (0.01/epsilon) ** (1/n_episodes)\n",
        "\n",
        "# Initialising pandas dataframes to store results\n",
        "# Only write data directly to the dataframe from the code below\n",
        "time_step_results = pd.DataFrame()\n",
        "\n",
        "# episode_result = pd.DataFrame(columns=ep_res_headers)\n",
        "episode_result = pd.DataFrame()\n",
        "\n",
        "\n",
        "# Begin the Training Loop\n",
        "stochasticity = 0.99\n",
        "max_timesteps = 20\n",
        "envir = GridEnvironment(observation_space, action_space, max_timesteps, stochasticity=stochasticity)\n",
        "obs = env.reset()\n",
        "\n",
        "for i in range(0,n_episodes):\n",
        "\n",
        "    envir.reset()\n",
        "\n",
        "    timestep_results, rewards_sum, is_episode_successful, steps, penalty, env = run_episode_q_learn(i, epsilon, envir)\n",
        "    time_step_results = time_step_results.append(timestep_results)\n",
        "\n",
        "    episode_result = episode_result.append({\n",
        "        'Episode #': i,\n",
        "        'Total Rewards': rewards_sum,\n",
        "        'Successful': is_episode_successful,\n",
        "        'Steps': steps,\n",
        "        'Penalty': penalty,\n",
        "        'Epsilon': epsilon\n",
        "    }, ignore_index=True)\n",
        "    epsilon *= decay_rate\n",
        "\n",
        "    envir = env\n",
        "\n",
        "    if i == n_episodes - 1:\n",
        "        print(\"Final q_table is: \\n\", q_table)\n",
        "\n",
        "timestep_file = './timestep_result_train_q_learn_deterministic.xlsx'\n",
        "episode_file = './episode_result_train_q_learn_deterministic.xlsx'\n",
        "\n",
        "if stochasticity < 1:\n",
        "    timestep_file = timestep_file.replace('deterministic', 'stochastic')\n",
        "    episode_file = episode_file.replace('deterministic', 'stochastic')\n",
        "\n",
        "time_step_results.to_excel(timestep_file)\n",
        "episode_result.to_excel(episode_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualise the agent's performance by running a single episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "envir.reset()\n",
        "evaluate_agent_qlearn(1, envir, print_episode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's evaluate the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_q_learn(envir, DETERMINISTIC=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementation of SARSA algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for SARSA\n",
        "\n",
        "# Function to get the max q value of the future state of the agent\n",
        "def get_sarsa_for_state(pos, future_action):\n",
        "    state = get_state_from_position(pos)\n",
        "    if state == env.observation_space.n - 1:\n",
        "        return 0\n",
        "    sarsa_list = sarsa_table[state]\n",
        "    return sarsa_list[future_action]\n",
        "\n",
        "# Function to calculate q_value\n",
        "def update_sarsa_value(current_pos, future_pos, immediate_reward, current_action, future_action):\n",
        "\n",
        "    state = get_state_from_position(current_pos)\n",
        "\n",
        "    if get_state_from_position(current_pos) == env.observation_space.n - 1:\n",
        "        q_table[state][current_action] = 0\n",
        "        return\n",
        "\n",
        "    s = sarsa_table[state][current_action] + learning_rate * ( immediate_reward + discount_factor * get_sarsa_for_state(future_pos, future_action) - sarsa_table[state][current_action])\n",
        "    sarsa_table[state][current_action] = s\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run episode for SARSA Learning\n",
        "def run_episode_sarsa(episode_num, epsilon):\n",
        "\n",
        "    rewards_sum = 0\n",
        "    timestep_results = []\n",
        "    \n",
        "    is_episode_successful = False\n",
        "    steps = 0\n",
        "    penalty = 0\n",
        "\n",
        "    current_action = None\n",
        "\n",
        "    for timestep in range(0,env.max_timesteps):\n",
        "\n",
        "        if current_action is None:\n",
        "            current_action = get_action()\n",
        "\n",
        "        current_pos = cp.deepcopy(env.agent_pos)\n",
        "        observation, reward, done, info, action = env.step(current_action)\n",
        "        future_pos = env.agent_pos\n",
        "\n",
        "        chosen_action = None\n",
        "        if np.random.random() < epsilon:\n",
        "            future_action = get_action()\n",
        "        else:\n",
        "            future_action = get_action_greedy(sarsa_table)\n",
        "        \n",
        "        if reward < 0:\n",
        "            penalty += 1\n",
        "        \n",
        "        # Update q table and current episode outcome\n",
        "        update_sarsa_value(current_pos, future_pos, reward, action, future_action)\n",
        "        rewards_sum += reward\n",
        "        steps += 1\n",
        "\n",
        "        current_action = future_action\n",
        "\n",
        "        timestep_results.append([episode_num, timestep, action, reward, done])\n",
        "        if done:\n",
        "            if steps < max_timesteps:\n",
        "                is_episode_successful = True\n",
        "            break\n",
        "            \n",
        "    return timestep_results, rewards_sum, is_episode_successful, steps, penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize SARSA learning parameters and panda dataframes for storing results\n",
        "sarsa_table = np.zeros((observation_space, action_space))\n",
        "learning_rate = 0.15\n",
        "epsilon = 1.0\n",
        "discount_factor = 0.9\n",
        "n_episodes = 1000\n",
        "decay_rate = 0.995\n",
        "\n",
        "# Only write data directly to the dataframe from the code below\n",
        "time_step_results = pd.DataFrame()\n",
        "\n",
        "# episode_result = pd.DataFrame(columns=ep_res_headers)\n",
        "episode_result = pd.DataFrame()\n",
        "\n",
        "# Function to run the Training Loop on SARSA algorithm\n",
        "\n",
        "max_timesteps = 15\n",
        "stochasticity = 1\n",
        "env = GridEnvironment(observation_space, action_space, max_timesteps, stochasticity=stochasticity)\n",
        "obs = env.reset()\n",
        "\n",
        "for i in range(0,n_episodes):\n",
        "\n",
        "    env.reset()\n",
        "\n",
        "    timestep_results, rewards_sum, is_episode_successful, steps, penalty = run_episode_sarsa(i, epsilon)\n",
        "    time_step_results = time_step_results.append(timestep_results)\n",
        "    \n",
        "    episode_result = episode_result.append({\n",
        "        'Episode #': i,\n",
        "        'Total Rewards': rewards_sum,\n",
        "        'Successful': is_episode_successful,\n",
        "        'Steps': steps,\n",
        "        'Penalty': penalty,\n",
        "        'Epsilon': epsilon\n",
        "    }, ignore_index=True)\n",
        "    epsilon *= decay_rate\n",
        "\n",
        "    if i == n_episodes - 1:\n",
        "        print(sarsa_table)\n",
        "\n",
        "timestep_file = './timestep_result_train_sarsa_deterministic.xlsx'\n",
        "episode_file = './episode_result_train_sarsa_deterministic.xlsx'\n",
        "\n",
        "if stochasticity < 1:\n",
        "    timestep_file = timestep_file.replace('deterministic', 'stochastic')\n",
        "    episode_file = episode_file.replace('deterministic', 'stochastic')\n",
        "\n",
        "time_step_results.to_excel(timestep_file)\n",
        "episode_result.to_excel(episode_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate agent on SARSA Learning\n",
        "def evaluate_agent_sarsa(num, print_episode=False):\n",
        "\n",
        "    penalty = 0\n",
        "    rewards_sum = 0\n",
        "    data_cell = []\n",
        "    steps = 0\n",
        "\n",
        "    for i in range(max_timesteps):\n",
        "        if print_episode:\n",
        "            env.render()\n",
        "        action = get_action_greedy(sarsa_table)\n",
        "        observation, reward, done, info, action = env.step(action)\n",
        "        if reward < 0:\n",
        "            penalty += 1\n",
        "\n",
        "        rewards_sum += reward\n",
        "        steps += 1\n",
        "        data_cell.append([num, i, action, reward, rewards_sum, done])\n",
        "        if done:\n",
        "            if print_episode:\n",
        "                env.render()\n",
        "            break\n",
        "        \n",
        "    return data_cell, rewards_sum, penalty, steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.reset()\n",
        "evaluate_agent_sarsa(1, print_episode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run the evaluation loop\n",
        "def eval_sarsa(DETERMINISTIC = True):\n",
        "    env.reset()\n",
        "    n_episodes = 10\n",
        "    timestep_data = []\n",
        "    episode_data = pd.DataFrame()\n",
        "\n",
        "    for i in range(1, n_episodes+1):\n",
        "        env.reset()\n",
        "        data_cell, rewards_sum, penalty, steps = evaluate_agent_sarsa(i)\n",
        "        timestep_data.append(data_cell)\n",
        "        episode_data = episode_data.append({\n",
        "            'Episode #': i, \n",
        "            'Total rewards': rewards_sum, \n",
        "            'Penalty': penalty,\n",
        "            'Steps': steps\n",
        "            }, ignore_index=True)\n",
        "\n",
        "        if i == n_episodes:\n",
        "            print(episode_data.head)\n",
        "\n",
        "    episode_file = './eval_ep_data_sarsa_deterministic.xlsx'\n",
        "    if not DETERMINISTIC:\n",
        "        episode_file = episode_file.replace('deterministic', 'stochastic')\n",
        "    episode_data.to_excel(episode_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_sarsa()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's train the agent on SARSA algorithm in a stochastic environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize SARSA learning parameters and panda dataframes for storing results\n",
        "sarsa_table = np.zeros((observation_space, action_space))\n",
        "learning_rate = 0.1\n",
        "epsilon = 1.0\n",
        "discount_factor = 0.9\n",
        "n_episodes = 2000\n",
        "decay_rate = (0.01/epsilon) ** (1/n_episodes)\n",
        "\n",
        "# Only write data directly to the dataframe from the code below\n",
        "time_step_results = pd.DataFrame()\n",
        "\n",
        "# episode_result = pd.DataFrame(columns=ep_res_headers)\n",
        "episode_result = pd.DataFrame()\n",
        "\n",
        "# Function to run the Training Loop on SARSA algorithm\n",
        "\n",
        "max_timesteps = 20\n",
        "stochasticity = 0.99\n",
        "env = GridEnvironment(observation_space, action_space, max_timesteps, stochasticity=stochasticity)\n",
        "obs = env.reset()\n",
        "\n",
        "for i in range(0,n_episodes):\n",
        "\n",
        "    env.reset()\n",
        "\n",
        "    timestep_results, rewards_sum, is_episode_successful, steps, penalty = run_episode_sarsa(i, epsilon)\n",
        "    time_step_results = time_step_results.append(timestep_results)\n",
        "    \n",
        "    episode_result = episode_result.append({\n",
        "        'Episode #': i,\n",
        "        'Total Rewards': rewards_sum,\n",
        "        'Successful': is_episode_successful,\n",
        "        'Steps': steps,\n",
        "        'Penalty': penalty,\n",
        "        'Epsilon': epsilon\n",
        "    }, ignore_index=True)\n",
        "    epsilon *= decay_rate\n",
        "\n",
        "    if i == n_episodes - 1:\n",
        "        print(sarsa_table)\n",
        "\n",
        "timestep_file = './timestep_result_train_sarsa_deterministic.xlsx'\n",
        "episode_file = './episode_result_train_sarsa_stochastic.xlsx'\n",
        "\n",
        "if stochasticity < 1:\n",
        "    timestep_file = timestep_file.replace('deterministic', 'stochastic')\n",
        "    episode_file = episode_file.replace('deterministic', 'stochastic')\n",
        "\n",
        "time_step_results.to_excel(timestep_file)\n",
        "episode_result.to_excel(episode_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.reset()\n",
        "evaluate_agent_sarsa(1, print_episode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's evaluate the agent ðŸ¤–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_sarsa(DETERMINISTIC=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spring22_lec_3_RL_Env.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
